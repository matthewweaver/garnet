import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
import numpy as np
import pandas as pd
from create_state_value_list import *
import time
import os
import configparser


# Generator Model
def build_generator():
    noise_input = layers.Input(shape=(latent_dim,))
    x = layers.Dense(32, activation="relu")(noise_input)
    x = layers.Dense(64, activation="relu")(x)
    action_output = layers.Dense(action_dim, activation="softmax")(x)

    return keras.Model(noise_input, action_output, name="Generator")


# Discriminator Model
def build_discriminator():
    action_input = layers.Input(shape=(action_dim,))
    value_input = layers.Input(shape=(action_dim,))
    x = layers.Concatenate()([action_input, value_input])
    x = layers.Dense(64, activation="relu")(x)
    x = layers.Dropout(0.3)(x)  # Add dropout to prevent overfitting
    x = layers.Dense(32, activation="relu")(x)
    x = layers.Dropout(0.3)(x)  # Add dropout to prevent overfitting
    validity_output = layers.Dense(1, activation="sigmoid")(x)

    return keras.Model(
        [action_input, value_input], validity_output, name="Discriminator"
    )


def build_gan(generator, discriminator):
    discriminator.trainable = False  # Freeze discriminator during generator training
    noise_input = layers.Input(shape=(latent_dim,))
    generated_action = generator(noise_input)
    validity = discriminator(
        [generated_action, keras.backend.stop_gradient(generated_action)]
    )

    return keras.Model(noise_input, validity, name="GAN")


# Generate synthetic training data
def generate_real_data(samples=1000):
    actions = np.eye(action_dim)[
        np.random.choice(action_dim, samples)
    ]  # One-hot encoded actions
    values = np.random.randint(0, 2, size=(samples, action_dim))  # Random binary values
    labels = (
        np.ones((samples, 1)) * 0.9
    )  # Real data label (1) with label smoothing applied for stability in training
    return actions, values, labels


def train(epochs=10000, batch_size=32, train_start_time=time.time()):
    for epoch in range(epochs):
        # Train discriminator
        real_actions, real_values, real_labels = generate_real_data(batch_size)
        noise = np.random.normal(0, 1, (batch_size, latent_dim))
        generated_actions = generator.predict(noise)

        # Add noise to real and fake actions to make it harder to overfit
        real_actions += np.random.normal(0, 0.05, real_actions.shape)
        generated_actions += np.random.normal(0, 0.05, generated_actions.shape)

        fake_values = np.random.randint(0, 2, size=(batch_size, action_dim))
        fake_labels = np.zeros((batch_size, 1))  # Fake data label (0)

        d_loss_real = discriminator.train_on_batch(
            [real_actions, real_values], real_labels
        )
        d_loss_fake = discriminator.train_on_batch(
            [generated_actions, fake_values], fake_labels
        )
        d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)

        # Train generator
        noise = np.random.normal(0, 1, (batch_size, latent_dim))
        g_loss = gan.train_on_batch(noise, np.ones((batch_size, 1)))

        if epoch % 100 == 0:
            print(
                f"Epoch {epoch}, D Loss: {d_loss[0]}, G Loss: {g_loss}, Time: {time.time() - train_start_time:.4f} seconds"
            )


# Custom loss function to avoid vanishing gradients
def wasserstein_loss(y_true, y_pred):
    return tf.reduce_mean(y_true * y_pred)


# Main function
if __name__ == "__main__":
    start_time = time.time()

    create_state_list()
    populate_action_values()

    # Define the list of possible outputs
    state_value_list = pd.read_csv("data/state_value_list.csv")

    # Read config.ini.
    config = configparser.ConfigParser()
    config.read("config.ini")

    # Hyperparameters
    action_dim = len(state_value_list)
    latent_dim = int(
        config["GAN"]["latent_dim"]
    )  # Small noise vector for stability in training
    batch_size = int(config["GAN"]["batch_size"])
    epochs = int(config["GAN"]["epochs"])

    # Instantiate models
    generator = build_generator()
    discriminator = build_discriminator()

    discriminator.compile(
        loss=wasserstein_loss,
        optimizer=keras.optimizers.Adam(
            learning_rate=0.0001, beta_1=0.5, clipvalue=1.0
        ),
        metrics=["accuracy"],
    )  # Discriminator given smaller learning rate to balance training

    gan = build_gan(generator, discriminator)
    gan.compile(
        loss=wasserstein_loss,
        optimizer=keras.optimizers.Adam(
            learning_rate=0.0002, beta_1=0.5, clipvalue=1.0
        ),
    )  # Added gradient clipping to prevent exploding gradients

    print(f"Execution time: {time.time() - start_time:.4f} seconds")

    train(epochs, batch_size)
    generator.save("gan/model/cGAN.keras")
